{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **INTRODUCTION TO MOBILE ROBOTICS PROJECT**\n",
    "##### MICRO-452"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Authors** : \n",
    "##### Laure Coquoz\n",
    "##### Hendrik Hilsberg\n",
    "##### Nathan Kammoun\n",
    "##### Adrien O'Hana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Table of contents \n",
    "\n",
    "* [0. Introduction](#Intro)\n",
    "* [I. Computer Vision](#c1)\n",
    "* [II. Global Navigation](#c2)\n",
    "* [III. Local Navigation](#c3)\n",
    "* [IV. Kalman Filter](#c4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Introduction <a class=\"anchor\" id=\"Intro\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We started our project by thinking about the design of our map. We decided to go with a white map, whose corners are represented by apriltags. We didn't define the edges of the map as a limit for the Thymio, and assumed that we would place the obstacles in a way that the Thymio always has enough space to pass between an obstacle and the edge of the map. Concerning the obstacles and the goal, we chose red cardboard pieces of various shapes for the former and a green dot for the latter. Finally, to localize the robot, we used a different apriltag than the one from the edges and placed it on top of the robot.\n",
    "\n",
    "Regarding the code, the main function is separated in two blocks: one for vision, and the other one for filtering and motion combined. We used a counter to define the ratio of executions of one block with regard to other. The vision block will be executed once, followed by ten executions of the block containing filtering and motion. These values were not chosen randomly : we timed each block to get an idea of the execution times. While the vision lasts approximately 150ms in the worst cases, the filtering and motion part combined last up to around 10ms. Using the above mentioned ratio means that there is a new vision step approximately four times every second. \n",
    "\n",
    "Here is a pseudo-code for a better comprehension of the main function:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "while true  \n",
    "    if count is equal to 0  \n",
    "        get the image from the webcam  \n",
    "        if thymio was detected  \n",
    "            update the position  \n",
    "        draw the map  \n",
    "        count + 1  \n",
    "    else  \n",
    "        if the last step was the vision block  \n",
    "            if the thymio was not detected  \n",
    "                predict position using odometry\n",
    "            else  \n",
    "                update position\n",
    "        else (last block wasn't vision)  \n",
    "            predict position using odometry \n",
    "        update goal(next waypoint)  \n",
    "        if final goal is reached  \n",
    "            set motors speed to zero  \n",
    "            break  \n",
    "        compute deviation to goal  \n",
    "        check sensor values(local obstacle)  \n",
    "        set motors speed(depending on deviation and presence of local obstacle)  \n",
    "        if count is equal to 10  \n",
    "            count = 0  \n",
    "        else  \n",
    "            count + 1  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# I. Vision <a class=\"anchor\" id=\"c1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Field Detection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "We wanted to keep the setup as easy to work with as possible. We knew we would have to re-install the whole setup a lot so we opted for the most flexible option in terms of camera positionning. Placing the camera on top of the field seems like the most straight-forward option but it also brings a lots of constraints. On top of that, we had to make sure that the webcam gives a consistent output each time we reinstalled the setup. That is why we opted for a convenient and flexible option in terms of webcam positionning : field detection. \n",
    "\n",
    "The field refers to the space where the thymio can go. We delimited it with four april tags at each corners. April tag is a visual fiducial system used in robotics, the AprilTag detection software computes the precise 3D position and orientation of the tags.\n",
    "\n",
    "A black color filtering is done on the image before corner detection in order to make AprilTag detection more efficient. Once the april tags are detected, the four corners positions are registered in order to later crop and transform each of the camera outputs. \n",
    "\n",
    "<p float=\"left\">\n",
    "  <img src=\"images/CameraFrame.jpeg\" width=\"500\" />\n",
    "</p>\n",
    "\n",
    "The detection of the corners is done in the **detect_corner_tags()** function in our code, using the **Detector** object of the **apriltag** librairy, the tags we used for the corners were the 36h11.\n",
    "\n",
    "The position of the four corners are passed to the **getPerspectiveTransform()** and **warpPerspective()** functions from **OpenCV** which results in a perspective transformed subset of the camera frame. Next, the result is resized to fit the real dimensions of the field. Our field size is **1050mmx1188mm**, we reisze the image to be of size **1050x1188** in order to get a 1 pixel/mm ratio. \n",
    "\n",
    "<p float=\"left\">\n",
    "  <img src=\"images/Field.jpeg\" width=\"400\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Bellow you can see the code of the main function used for field detection. The code access the **crop** attribute which contains the positions of each corners of the field* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_field(self, image) : \n",
    "\n",
    "    \"\"\"\n",
    "    Goal:  Crop the camera frame to fit the thymio field \n",
    "    Input:  image = camera frame \n",
    "    Output: field = cropped and transformed camera frame to fit the thymio field \n",
    "    \"\"\"\n",
    "\n",
    "    # Field Size \n",
    "    width  = self.field_size['width']\n",
    "    height = self.field_size['height']\n",
    "\n",
    "    # Image \n",
    "    rows,cols,ch = image.shape\n",
    "    pts1 = np.float32([self.crop['top_left'], self.crop['top_right'], self.crop['bottom_left'], self.crop['bottom_right']])\n",
    "    pts2 = np.float32([[0,0],[width, 0],[0, height],[width,height]])\n",
    "    M = cv2.getPerspectiveTransform(pts1,pts2)\n",
    "    field = cv2.warpPerspective(image, M, (width,height))\n",
    "\n",
    "    return field "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thymio Detection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used another AprilTag to detect the thymio. The good thing being that april tag detection gives us the position of each one of the detected april tags corners. We compute the orientation of the tag by considering the angle between its center and the middle point between its top left and top right corners.\n",
    "\n",
    "*Note : The output of the thymio detection is the position of the thymio tag. In the Robot class the actual position of the thymio is computed by translating the thymio tag center position along its orientation.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Bellow you can see the code of the main function used in thymio detection* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thymio_detection(self, field) : \n",
    "    \"\"\"\n",
    "    Goal:  Detect the thymio tag  \n",
    "    Input: field = cropped and transformed camera frame to fit the thymio field \n",
    "    Output: thymio tag = dictionnary with positions of thymio tag center and 'nose' \n",
    "            -- The thymio tag 'nose' is the middle position between its top right and top left corners --\n",
    "    \"\"\"\n",
    "    \n",
    "    # Thymio Tag Position \n",
    "    thymio_tag = {'pos' : None, 'nose': None}\n",
    "\n",
    "    # Tag Detection \n",
    "    options  = apriltag.DetectorOptions(families=\"tag25h9\")\n",
    "    detector = apriltag.Detector(options)\n",
    "    results  = detector.detect(cv2.cvtColor(field, cv2.COLOR_BGR2GRAY)) \n",
    "\n",
    "    # Results \n",
    "    if len(results)==0 : \n",
    "        print('Thymio Not Detected')\n",
    "        return None\n",
    "    else :   \n",
    "        r = results[0]\n",
    "        (ptA, ptB, ptC, ptD) = r.corners\n",
    "        ptB = (int(ptB[0]), int(ptB[1]))\n",
    "        ptC = (int(ptC[0]), int(ptC[1]))\n",
    "        ptD = (int(ptD[0]), int(ptD[1]))\n",
    "        ptA = (int(ptA[0]), int(ptA[1]))\n",
    "\n",
    "        thymio_tag['pos']  = (int(r.center[0]), int(r.center[1]))\n",
    "        thymio_tag['nose'] = (int((ptA[0]+ptB[0])/2), int((ptA[1]+ptB[1])/2))\n",
    "        return thymio_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obstacle Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to plan a path for the robot we need to compute the position of each one of the obstacle within the field, to be precise we need the positions of each corners of each obstacles.  \n",
    "\n",
    "The obstacles were chosen to be red pieces of paper on the field. We first perform a filtering on red values which filters the obstacles on the thymio field image. The *red_filtering* function filters all values within a given RGB range (the threshold are set manually given obstacles color).\n",
    "\n",
    "<p float=\"left\">\n",
    "  <img src=\"images/RedFiltering.jpeg\" width=\"300\" />\n",
    "</p>\n",
    "\n",
    "One thing we had to take into account is that we only detect the thymio center, which does not reflect the size of the entire robot. The stategy we used was to increase the obstacle size to account for the actual size of the robot. This is done through a sliding kernel on the filtered red values, it results in larger obstacles. This is done thanks to the **dilate()** function from OpenCV. \n",
    "\n",
    "A combination of two functions is used to compute the corners of each augmented obstacles. The **findContours()** function computes the contours of obstacles given the dilated output of the red filtering pass. Then the **approxPolyDP()** function fits a polygon from the contours and outputs the corners of these polygons. \n",
    "\n",
    "*Note : The detection of rounded shapes is not a problem as they are just interprated as polygons with a high number of corners*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Bellow you can see the code of the main function used for obstacles detection* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obstacles_detection(self, field, thymio_clearance=120) :     \n",
    "        \"\"\"\n",
    "        Goal:  Detected obstacles \n",
    "        Input: field = cropped and transformed camera frame to fit the thymio field \n",
    "               thymio_clearance = how much the obstacles should be increased considering the thymio size (mm) \n",
    "        Output: obstacles_dict = dictionnary of obstacles corners positions \n",
    "                dilated_obstacles_dict = dictionnary of dilated obstacles corners positions \n",
    "        \"\"\"\n",
    "        # Store Obstacle Shapes\n",
    "        obstacles_dict = {}\n",
    "        dilated_obstacles_dict = {}\n",
    "        \n",
    "        # Get obstacles \n",
    "        red_filter = self.red_filtering(field)\n",
    "        \n",
    "        # Bitwise Inverse\n",
    "        obstacles = cv2.bitwise_not(red_filter[:, :, 0])\n",
    "        \n",
    "        # Expand Obstacles\n",
    "        kernel = np.ones((thymio_clearance, thymio_clearance), np.uint8)\n",
    "        dilated_obstacles = cv2.dilate(obstacles, kernel, iterations=1)\n",
    "        \n",
    "        # Find contours of filtered shapes\n",
    "        contours, _  = cv2.findContours(obstacles, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        dilated_contours, _ = cv2.findContours(dilated_obstacles, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        # Compute corners from contours \n",
    "        for i, contour in enumerate(contours):\n",
    "            # Approximate Shape \n",
    "            approx = cv2.approxPolyDP(contour, 0.01*cv2.arcLength(contour, True), True)\n",
    "            obstacles_dict[i] = [(corners[0][0], corners[0][1]) for corners in approx]\n",
    "         \n",
    "        # Compute corners from contours\n",
    "        for j, contour in enumerate(dilated_contours):\n",
    "            # Approximate Shape \n",
    "            approx = cv2.approxPolyDP(contour, 0.01*cv2.arcLength(contour, True), True)\n",
    "            dilated_obstacles_dict[j] = [(corners[0][0], corners[0][1]) for corners in approx]\n",
    "            \n",
    "        return obstacles_dict, dilated_obstacles_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal Detection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal was chosen to be a green piece of paper of any shape. First, a filtering on green values was performed to filter the goal on the thymio field, the *green_filtering* function filters all values within a given RGB range (the threshold are set manually given goal color).\n",
    "\n",
    "The **findContours()** function computes the contours of the goal, then the **moments()** function enables to find the center of the goal contour and make it the detected goal point.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Bellow you can see the code used for goal detection* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def goal_detection(self, field) : \n",
    "    \"\"\"\n",
    "    Goal:  Detected obstacles \n",
    "    Input: field = cropped and transformed camera frame to fit the thymio field \n",
    "    Output: goal = dictionnary containing position of the goal, position is set to origin in case of detection failure\n",
    "    \"\"\"\n",
    "    # Goal Position \n",
    "    goal = {}\n",
    "\n",
    "    # Filtering of green values\n",
    "    green_filter = self.green_filtering(field)\n",
    "    contours, _  = cv2.findContours(green_filter, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Test detection \n",
    "    if len(contours) == 1  : \n",
    "        print('Goal Not Detected')\n",
    "        goal['pos'] = (0, 0)\n",
    "        return goal\n",
    "    else :          \n",
    "        # Get center of green shape\n",
    "        M = cv2.moments(contours[1])\n",
    "        cX = int(M[\"m10\"] / M[\"m00\"])\n",
    "        cY = int(M[\"m01\"] / M[\"m00\"]) \n",
    "        goal['pos'] = (cX, cY)\n",
    "\n",
    "        return goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# II. Global Navigation <a class=\"anchor\" id=\"c2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Path Planning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The global path is computed once as soon as we detect the first thymio position. The goal is detected and obstacle detection is performed : we have a goal point, a list of corners for each obstacles as well as the robot position. The global path planning approach we chosed was to compute the shortest path thanks to a visibility graph.\n",
    "\n",
    "The visibily graph was easily computed thanks to the **PyVisGraph** librairy.\n",
    "\n",
    "*Below the code used to compute the global path*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_planning(self, field_map, final_goal, obstacles) : \n",
    "    \"\"\"\n",
    "    Goal : Calculate the shortest path between the robot and the goal\n",
    "    Input : field_map 2D array with shape of the size of the map in mm, final_goal['pos'] = (x,y), obstacles dictionary \n",
    "    Output : field_map with added path\n",
    "    \"\"\"                                   \n",
    "    # Definition \n",
    "    start = vg.Point(self.x_est, self.y_est)\n",
    "    objective  = vg.Point(final_goal['pos'][0], final_goal['pos'][1])\n",
    "\n",
    "    # Calculate shortest path \n",
    "    polys = [[vg.Point(corners[0], corners[1]) for corners in obstacle] for (i, obstacle) in obstacles.items()]\n",
    "    g = vg.VisGraph()\n",
    "    g.build(polys)\n",
    "    shortest = g.shortest_path(start, objective)\n",
    "\n",
    "    # Define Path and Next Goal in Graph  \n",
    "    self.path = shortest\n",
    "    self.next_goal = 1\n",
    "\n",
    "    # Update Field Map \n",
    "    field_map = self.draw_path(field_map)\n",
    "\n",
    "    return field_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Following the way "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The path planning gives the thymio a road map it has to follow, a series of goal it has to reach and between which it ideally has to navigate in straight lines. The thymio updates the position of its next goal when reaching one until it reaches the final destination. As it is not likely that the robot position coincides exactly with the goal position, the updates happens once the thymio is sufficiently close from its goal. \n",
    "\n",
    "*Below the code used to handle navigation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_next_goal(self, tolerance=30, verbose=True) : \n",
    "    \"\"\"\n",
    "    Goal : Check distance with current goal and updates it if it comes sufficiently close\n",
    "    Output : Return True if Goal Updated and False if not \n",
    "    \"\"\"         \n",
    "    # Get next waypoint and calculate its distance to the robot\n",
    "    goal_pos = self.path[self.next_goal]\n",
    "    distance2goal = math.sqrt((self.x_est-goal_pos.x)**2+(self.y_est-goal_pos.y)**2)\n",
    "\n",
    "    # Update the next waypoint if the robot is close enough to the next waypoint\n",
    "    if distance2goal < tolerance : \n",
    "        if verbose : print('Reached Goal {0}'.format(self.next_goal))\n",
    "        if self.next_goal == len(self.path)-1 : \n",
    "            self.finished=True\n",
    "            if verbose : print('Your journey is over little thymio')\n",
    "        else :\n",
    "            self.next_goal =  self.next_goal+1 \n",
    "            if verbose : print('Now directing towards {0}'.format(self.next_goal))\n",
    "        return True \n",
    "    else :\n",
    "        return False "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We thought about stopping when reaching each intermediate goal and turning towards the next one before moving but we opted for a more dynamic behavior. Our strategy allows the thymio to deviate from the planned path while constantly redirecting itself towards its next goal. \n",
    "\n",
    "- Green Lines: Planned path \n",
    "- Yellow Dots : Intermediate goals \n",
    "- Orange Line : Thymio to goal direction\n",
    "\n",
    "<img src=\"images/GlobalNav.png\" width=\"600\" height=\"400\">\n",
    "\n",
    "The deviation is computed by computing the difference between the thymio orientation and the direction towards its next goal. Then the deviation angle is multiplied by a deviation gain that is used to update the motors input.\n",
    "\n",
    "$ L_{speed_{goal}} = Gain_{deviation} * deviation\\_angle$  \n",
    "$ R_{speed_{goal}} = Gain_{deviation} * deviation\\_angle$\n",
    "\n",
    "*Below the code used to compute the deviation angle*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_deviation(self) : \n",
    "    '''\n",
    "    Goal : Compute angle difference between thymio and goal to use it in naviguation\n",
    "    Output : Deviation angle (rad)\n",
    "    '''\n",
    "    #Calculate angle between robot and next waypoint\n",
    "    goal_pos = self.path[self.next_goal]\n",
    "    omega = math.atan2(goal_pos.y-self.y_est, goal_pos.x-self.x_est)       \n",
    "\n",
    "    #Calculate difference with robot orientation and adjust angle to be in the interval [-pi,pi]\n",
    "    if omega-self.theta_est<-np.pi :\n",
    "        dev=-(omega-self.theta_est+np.pi)\n",
    "    elif omega-self.theta_est>np.pi : \n",
    "        dev=-(omega-self.theta_est-np.pi)\n",
    "    else :\n",
    "        dev=omega-self.theta_est\n",
    "        \n",
    "    return dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real Time Vizualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a way to check that everything is working correctly we implemented a real time vision of the field. \n",
    "This additional feature aims at providing a better insight on what is going on during the robot's journey. First, field detection is performed, then obstacles and goal detection follow. As soon as the thymio is detected, global path planning is performed and the shortest path is added to the map. \n",
    "\n",
    "Once this initial mapping is done, we update the map at a given frequency to update the robot's position along the way. \n",
    "\n",
    "- The green dot shows the goal detected position \n",
    "- The orange dots and lines represents the augmented obstacles\n",
    "- The **orange** dots represent each one of the point that the robots has to reach along the way. \n",
    "- The **yellow** lines represent the straight lines the robot has to follow to reach the next goal-point. \n",
    "- The blue dot represents the position od the thymio tag if it is detected \n",
    "- The pink line shows its orientation \n",
    "- The pink dot shows the position of the robot\n",
    "\n",
    "<p float=\"left\">\n",
    "  <img src=\"images/Visualisation.jpeg\" width=\"400\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# III. Local Navigation <a class=\"anchor\" id=\"c3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a simple way to add local navigation to the existing structure of the project without having to implement a state machine : **Potential Field Navigation**. \n",
    "The idea is to compute speed components for the left and right wheels depending on the front sensor values. Those speed components are then added to the ones calucated to reach the next waypoint. Therefore, if there is a new obstacle in the way the robot will simply adjust its trajectory to avoid the obstacle while still going towards the next waypoint. \n",
    "\n",
    "Additionaly we've implemented a form of memory so that the robot remembers that it is avoiding a local obstacle even when the sensors have just stopped seeing it. In order to do this the previous left and right wheel speed components for local avoidance are added to the new ones with a decay, so eventually these components reach null values shortly after there is no more obstacle in sight.  \n",
    "\n",
    "Here are the formulas implemented the code below :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ L_{speed} = L_{speed_{goal}} + L_{speed_{obstacle}} $  \n",
    "$ R_{speed} = R_{speed_{goal}} + R_{speed_{obstacle}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ L_{speed_{obstacle}} = prox\\_values_{front} \\cdot weight\\_matrix_{left} $  \n",
    "$ R_{speed_{obstacle}} = prox\\_values_{front} \\cdot weight\\_matrix_{right} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in the practicals, we extend the array containing the proximity sensor values (**x**) so it can also record the previous values for left and right speeds due to the obstacle, and that is the memory. These values stored in **x[5]** and **x[6]** are divided by a number greater than 1 we call **mem_decay** so the memory component diminishes with time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deviation = robot.compute_deviation()\n",
    "prox_values = get_prox_values()\n",
    "\n",
    "#speed to reach goal\n",
    "L_speed_goal = speed + devGain * deviation\n",
    "R_speed_goal = speed - devGain * deviation\n",
    "\n",
    "#store front sensor values\n",
    "for i in range(5):\n",
    "    # Get and scale inputs\n",
    "    x[i] = prox_values[i]//sensor_scale\n",
    "\n",
    "#Memory\n",
    "x[5] = L_speed_obstacle//mem_decay\n",
    "x[6] = R_speed_obstacle//mem_decay\n",
    "\n",
    "#Reset obstacle speeds after storing\n",
    "L_speed_obstacle = 0\n",
    "R_speed_obstacle = 0\n",
    "\n",
    "#compute outputs of neurons\n",
    "for i in range(len(x)):    \n",
    "    L_speed_obstacle += x[i] * w_l[i]\n",
    "    R_speed_obstacle += x[i] * w_r[i]\n",
    "\n",
    "#sum speeds to reach goal and to avoid obstacle\n",
    "L_speed = L_speed_goal + L_speed_obstacle\n",
    "R_speed = R_speed_goal + R_speed_obstacle\n",
    "\n",
    "#apply new speed to motors\n",
    "set_motors(int(L_speed), int(R_speed))\n",
    "robot.L_speed = L_speed\n",
    "robot.R_speed = R_speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of local obstacle avoidance can be seen in the following video : https://drive.google.com/drive/folders/1v1ewrE3aYPwxLCQqBAzRLtnIOmm1f-SC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# IV. Kalman filter <a class=\"anchor\" id=\"c4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the robot is moving on the map, it uses the speed measurements of its motors and the position given by the webcam to localize itself. Unfortunately this data is subject to noise which can induce error. Indeed, the speeds that we get from the wheel motors are not exactly the ones that we set due to perturbations on the measurements and due to the nature of the physical world. Additionally the state that we capture with the webcam can not be considered as ground truth and has an uncertainty due to discretization. \n",
    "\n",
    "In order to better estimate the position and orientation of the robot, sensor Fusion can be done. This can be achieved with the use of a Kalman Filter which will furthermore allow the robot to localize itself even if one of the data sources is momentarily missing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we choose to use the Extended Kalman Filter that is a Kalman Filter applied to a non linear system linearized at different discrete time. It uses measurements observed over time, statistical noise and other inaccuracies of the system to produce an estimation of the state. With these estimations, it is possible to be more accurate than by trusting only on the measurements. \n",
    "\n",
    "The Kalman Filter uses two different phases: the prediction phase and the measurement update phase. While we have no new measurement given by the camera we use the prediction step. If we have just received a new measurement from the camera, we use the update step. These steps give an estimation of the state of the robot that we have chosen to define as $(x, y, \\theta)$.\n",
    "\n",
    "The following snippet shows the conditions on which one step or the other is done, which depends on the boolean **boolKF** set to **True** every time a new state is retrieved from the camera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------ FILTERING ---------------------------\n",
    "\n",
    "# When the vision block is not \"ON\"\n",
    "else :\n",
    "    # Get the value of the motors of the Thymio\n",
    "    [L_speed,R_speed] = get_motors()      \n",
    "\n",
    "    if boolKF is True:\n",
    "        # Last block was the vision \n",
    "        if thymio_tag is None:\n",
    "            # The Thymio tag is not detected so we predict the Thymio's state from the odometry\n",
    "            kalman.kalman_prediction(robot,L_speed,R_speed)\n",
    "            # Indicate that last block was not vision (Kalman_prediction)\n",
    "            boolKF = False\n",
    "        else:\n",
    "            # The Thymio tag is detected so we can update the Thymio's state with the webcam\n",
    "            # Indicate that last block was not vision (Kalman_update)\n",
    "            boolKF = kalman.kalman_update(robot)\n",
    "\n",
    "    elif boolKF is False:\n",
    "        # Last block was the vision so we predict the Thymio's state with the webcam\n",
    "        kalman.kalman_prediction(robot,L_speed,R_speed)\n",
    "        # Indicate that last block was not vision (Kalman_prediction)\n",
    "        boolKF = False\n",
    "\n",
    "\n",
    "    #Update Goal \n",
    "    robot.update_next_goal(tolerance=10)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction phase is applied when the system does not have the value of the measurement of webcam. We use the webcam as the ground truth and when we do not have access to it, the Kalman Filter produces an estimation of the current state variables and its uncertainties. The state is predicted from the odometry formulas (in matrix **B**). We use the data from motion sensors to estimate the change in position over time. The formula used is:\n",
    "-  $E = AE+BU \\cdot deltaT$\n",
    "\n",
    "Where E is the state vector containing $(x, y, \\theta)$, A is the transition matrix of the state (Identity here), B is the transition matrix of the input based on the odometry formulas, U the motor speeds of the left and the right wheels and deltaT is the time that passed between the last Kalman prediction.\n",
    "\n",
    "Once the prediction of the state is done, this phase update also the uncertainties of the system. The formula used is the following:\n",
    "- $Q = BU_{var}B^T$\n",
    "- $P = APA^T + Q$\n",
    "\n",
    "Where Q contains the uncertainties due to the speed of the motors, $U_{var}$ is the variation of the speed of the motors and P contains the uncertainties of the system. The uncertainty of the system P is used in the second phase, the measurement update. \n",
    "\n",
    "The variances of the left and right speeds was calculated with our own robot giving it a fixed speed of 50 (our operating speed) and measuring the resultings left and right speeds many times over a long period. The result of this experiment gave us a rounded value of 19 which is then converted to _mm_ using **B**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : For better tracking in the direction perpendicular to the wheels, we decided to add additional uncertainty as a practical fix. This explains the addition of an identity matrix in the formula computing Q, which actually adds uncertainty in the three state variables in case we want to pick up the robot and displace it. In the following videos, we show how the robot is tracked with and without this additional uncertainty.  (https://drive.google.com/drive/folders/1iKniy9w9gv823rdwRVa8SnVvJ6L8U6Rl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, you can see the code used to perform the prediction function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kalman_prediction(self,thymio,L_speed,R_speed):\n",
    "\n",
    "    \"\"\"\n",
    "    Goal:     Since at this point we do not have the measurement done by the webcam, it\n",
    "              predicts the state of the robot based on the odometry and it updates the \n",
    "              variances of the system.\n",
    "    Input:    thymio = thymio class with the data of our robot\n",
    "              L_speed = speed of the left wheel applied until now\n",
    "              R_speed = speed of the right wheel applied until now\n",
    "    Output:   -\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute the time between the last update/prediction and this time\n",
    "    deltaT = time.time_ns()/10e8 - self.lastKalman\n",
    "    # Compute the transition matrix of the input corresponding to the odometry formula\n",
    "    self.B = np.matrix([[0.5*math.cos(self.E[2])*self.c,0.5*math.cos(self.E[2])*self.c],\n",
    "                        [0.5*math.sin(self.E[2])*self.c,0.5*math.sin(self.E[2])*self.c],\n",
    "                        [1/self.b*self.c, -1/self.b*self.c]],dtype= 'float')\n",
    "\n",
    "    # Compute the predicted state of the robot (AE+BU)\n",
    "    self.E = np.dot(self.A, self.E) + np.dot(self.B, self.U)*deltaT\n",
    "\n",
    "    # Compute the additionnal uncertainty due to the motors\n",
    "    Q = np.dot(self.B, np.dot(self.U_var_m,self.B.T))+np.eye(3)\n",
    "    # Update the variance of the system\n",
    "    self.P = np.dot(np.dot(self.A,self.P),np.transpose(self.A))+Q\n",
    "\n",
    "    # Update the state with the predicted x, y and theta\n",
    "    thymio.x_est = self.E[0].item()\n",
    "    thymio.y_est = self.E[1].item()\n",
    "    thymio.theta_est = self.E[2].item()\n",
    "\n",
    "    # Keep the speeds of the robot in U to compute the next prediction\n",
    "    self.U = np.matrix([[L_speed],[R_speed]],dtype= 'float')\n",
    "\n",
    "    # Update the time of the last kalman done to find deltaT\n",
    "    self.lastKalman = time.time_ns()/10e8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measurement update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the measurement of the webcam is done, the Kalman filter directly applies its measurement update function. Using the new state value given by the camera and the uncertainty matrix P computed in the previous function, it corrects the state of the robot. The formulas used in this phase are:\n",
    "- $K= (P*H^T) / (H*P*H^T +R)$\n",
    "- $E = E+(K*(Z-H*E))$\n",
    "- $P = (I-K*H)*P$\n",
    "\n",
    "Where H is the measurement matrix, here the identity because the webcam data has already been converted to state variables. K is the Kalman gain that specifies the degree to which the measurement is incorporated into the new estimation of the state. R is the matrix with the uncertainties of the measurements. \n",
    "\n",
    "In this project, the uncertainty of the x and y values is the size of one pixel (1mm) and 1 degree for $\\theta$. P is then updated here since we have to add the uncertainties of the measurement. \n",
    "\n",
    "Below, you can see the code used to perform the measurement update function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kalman_update(self,thymio):\n",
    "    \"\"\"\n",
    "    Goal:   Since we have the measurement of the camera, it can update \n",
    "            the state of the robot and update the variances of the system.\n",
    "    Input:  thymio = thymio class with the data of our robot\n",
    "\n",
    "    Output: returns false to set the boolean that says that the last block \n",
    "            was not the vision block.\n",
    "    \"\"\"\n",
    "\n",
    "    # Put the measurement of the webcam in the measured state matrix\n",
    "    Z = np.matrix([[thymio.x_cam],[thymio.y_cam],[thymio.theta_cam]],dtype= 'float')\n",
    "\n",
    "    # Update the state of the kalman with the state of the robot\n",
    "    self.E[0] = thymio.x_est \n",
    "    self.E[1] = thymio.y_est \n",
    "    self.E[2] = thymio.theta_est \n",
    "\n",
    "    # Computation of the Kalman gain \n",
    "    K_den = np.linalg.inv(np.dot(self.H,np.dot(self.P,np.transpose(self.H))) + self.R)\n",
    "    K_num = np.dot(self.P,np.transpose(self.H))\n",
    "    K = np.dot(K_num,K_den)        \n",
    "\n",
    "    #Correction of the state with the Kalman gain and the measurements\n",
    "    self.E = self.E + np.dot(K,(Z-np.dot(self.H,self.E)))\n",
    "\n",
    "    #Update of the variance of the system\n",
    "    I = np.eye(3)\n",
    "    self.P = np.dot((I-np.dot(K,self.H)),self.P)\n",
    "\n",
    "    # Update the state with the corrected x, y and theta\n",
    "    thymio.x_est = self.E[0].item()\n",
    "    thymio.y_est = self.E[1].item()\n",
    "    thymio.theta_est = self.E[2].item()\n",
    "\n",
    "    # Update the time of the last kalman done to find deltaT\n",
    "    self.lastKalman = time.time_ns()/10e8\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Conclusion & Demonstration\n",
    "\n",
    "Coordinating and joining the differents parts of this project has been a real challenge and we are proud to have successful results that we have demonstrated and recorded in the link below. \n",
    "\n",
    "In these videos we can see the robot and the map through a phone camera as well as the extracted information from the vision which has been screen recorded on the computer. We demonstrate global navigation through multiple obstacles and local navigation with an added obstacle. We also demonstrate that the estimation of the Kalman filter is working even when the camera is hidden for a short time.\n",
    "\n",
    "https://drive.google.com/drive/folders/1_lI-Vve-SCC8VqV3Ai6MmwPxx2l8Qoub?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Sources : \n",
    "\n",
    "#### Apriltag : \n",
    "https://april.eecs.umich.edu/software/apriltag#:~:text=AprilTag%20is%20a%20visual%20fiducial,tags%20relative%20to%20the%20camera. \n",
    "\n",
    "#### OpenCV :\n",
    "https://opencv.org/ \n",
    "\n",
    "#### Pyvisgraph : \n",
    "https://github.com/TaipanRex/pyvisgraph \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
